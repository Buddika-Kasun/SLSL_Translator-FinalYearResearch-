{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c8e381",
   "metadata": {},
   "source": [
    "# ===============================================================\n",
    "# üìó NOTEBOOK 2: Data Preparation & Analysis\n",
    "# Load landmarks, prepare sequences, create train/val/test splits\n",
    "# ==============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb07c8a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1: Setup ---\n",
    "\n",
    "!pip install tensorflow pandas numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1329735",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 2: Load Metadata ---\n",
    "\n",
    "def load_dataset(landmarks_path='/content/landmarks_data',\n",
    "                 metadata_path='/content/metadata/sentence_dataset_metadata.csv'):\n",
    "    \"\"\"Load landmarks and metadata\"\"\"\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata = pd.read_csv(metadata_path, encoding='utf-8-sig')\n",
    "    print(f\"üìä Metadata loaded: {len(metadata)} entries\")\n",
    "    \n",
    "    # Filter successful\n",
    "    success_df = metadata[metadata['success'] == True]\n",
    "    print(f\"‚úÖ Successful: {len(success_df)} videos\")\n",
    "    \n",
    "    # Load sentence mapping\n",
    "    try:\n",
    "        with open('/content/metadata/sentence_mapping.json', 'r', encoding='utf-8') as f:\n",
    "            sentence_mapping = json.load(f)\n",
    "        print(f\"üìù Sentence mapping loaded: {len(sentence_mapping)} sentences\")\n",
    "    except:\n",
    "        sentence_mapping = None\n",
    "        print(\"‚ö†Ô∏è No sentence mapping found\")\n",
    "    \n",
    "    return success_df, sentence_mapping\n",
    "\n",
    "# Load data\n",
    "success_df, sentence_mapping = load_dataset()\n",
    "print(f\"\\nüìã First 5 entries:\")\n",
    "print(success_df[['sentence', 'signer_id', 'left_hand_coverage']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c46eb7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 3: Load Landmark Sequences ---\n",
    "\n",
    "def load_landmark_sequences(metadata_df, landmarks_folder='/content/landmarks_data'):\n",
    "    \"\"\"Load all landmark sequences into memory\"\"\"\n",
    "    \n",
    "    X = []  # Features\n",
    "    y = []  # Labels (sentences)\n",
    "    signer_ids = []  # Signer info for split\n",
    "    \n",
    "    print(\"üîÑ Loading landmark sequences...\")\n",
    "    \n",
    "    for idx, row in metadata_df.iterrows():\n",
    "        try:\n",
    "            # Load .npy file\n",
    "            file_path = os.path.join(landmarks_folder, row['landmarks_file'])\n",
    "            landmarks_data = np.load(file_path, allow_pickle=True)\n",
    "            \n",
    "            # Convert to feature matrix\n",
    "            features = []\n",
    "            for frame in landmarks_data:\n",
    "                frame_features = []\n",
    "                \n",
    "                # Left hand (if available)\n",
    "                if frame['left_hand'] is not None:\n",
    "                    frame_features.extend(frame['left_hand'])\n",
    "                else:\n",
    "                    frame_features.extend([0.0] * (21 * 4))  # 21 points √ó 4 values\n",
    "                \n",
    "                # Right hand\n",
    "                if frame['right_hand'] is not None:\n",
    "                    frame_features.extend(frame['right_hand'])\n",
    "                else:\n",
    "                    frame_features.extend([0.0] * (21 * 4))\n",
    "                \n",
    "                # Pose (upper body)\n",
    "                if frame['pose'] is not None:\n",
    "                    frame_features.extend(frame['pose'])\n",
    "                else:\n",
    "                    frame_features.extend([0.0] * (25 * 4))  # 25 points √ó 4 values\n",
    "                \n",
    "                # Lip ROI\n",
    "                if frame['lip_roi'] is not None:\n",
    "                    frame_features.extend(frame['lip_roi'])\n",
    "                else:\n",
    "                    frame_features.extend([0.0] * (50 * 4))  # ~50 lip points √ó 4 values\n",
    "                \n",
    "                features.append(frame_features)\n",
    "            \n",
    "            X.append(np.array(features))\n",
    "            y.append(row['sentence'])\n",
    "            signer_ids.append(row['signer_id'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {row['landmarks_file']}: {e}\")\n",
    "    \n",
    "    X = np.array(X)\n",
    "    print(f\"\\n‚úÖ Loaded {len(X)} sequences\")\n",
    "    print(f\"   Shape: {X.shape}\")\n",
    "    print(f\"   Features per frame: {X.shape[2]}\")\n",
    "    \n",
    "    return X, np.array(y), np.array(signer_ids)\n",
    "\n",
    "# Load all data\n",
    "X, y, signer_ids = load_landmark_sequences(success_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6380847a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 4: Analyze Class Distribution ---\n",
    "\n",
    "def analyze_class_distribution(y_labels):\n",
    "    \"\"\"Analyze distribution of sentences\"\"\"\n",
    "    \n",
    "    # Count occurrences\n",
    "    counter = Counter(y_labels)\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_items = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nüìä CLASS DISTRIBUTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for sentence, count in sorted_items:\n",
    "        percentage = (count / len(y_labels)) * 100\n",
    "        print(f\"{sentence[:30]:30} : {count:3} videos ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sentences = [s[:20] + '...' for s, _ in sorted_items[:15]]\n",
    "    counts = [c for _, c in sorted_items[:15]]\n",
    "    \n",
    "    plt.barh(range(len(sentences)), counts)\n",
    "    plt.yticks(range(len(sentences)), sentences)\n",
    "    plt.xlabel('Number of Videos')\n",
    "    plt.title('Top 15 Sentences by Video Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/class_distribution.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    return counter\n",
    "\n",
    "counter = analyze_class_distribution(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26620945",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 5: Encode Labels ---\n",
    "\n",
    "def encode_labels(y_labels):\n",
    "    \"\"\"Convert sentence strings to numeric labels\"\"\"\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_labels)\n",
    "    \n",
    "    # Create mapping\n",
    "    label_mapping = dict(zip(label_encoder.classes_, \n",
    "                              label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    print(f\"\\nüî¢ Label Encoding:\")\n",
    "    print(f\"   Classes: {len(label_mapping)}\")\n",
    "    print(f\"   Sample mapping:\")\n",
    "    for sentence, idx in list(label_mapping.items())[:5]:\n",
    "        print(f\"   {sentence[:30]:30} ‚Üí {idx}\")\n",
    "    \n",
    "    # Save mapping\n",
    "    with open('/content/label_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    with open('/content/label_mapping.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(label_mapping, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return y_encoded, label_encoder\n",
    "\n",
    "y_encoded, label_encoder = encode_labels(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad342e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 6: Create Train/Val/Test Splits (Signer Independent) ---\n",
    "\n",
    "def create_signer_independent_splits(X, y, signer_ids, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Create splits where signers in test set are NOT in training set\n",
    "    This tests real-world generalization\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_signers = np.unique(signer_ids)\n",
    "    print(f\"\\nüë• Unique signers: {unique_signers}\")\n",
    "    \n",
    "    # Split signers into train/val/test\n",
    "    n_signers = len(unique_signers)\n",
    "    n_test = int(n_signers * test_size)\n",
    "    n_val = int(n_signers * val_size)\n",
    "    n_train = n_signers - n_test - n_val\n",
    "    \n",
    "    # Shuffle signers\n",
    "    np.random.seed(42)\n",
    "    shuffled_signers = np.random.permutation(unique_signers)\n",
    "    \n",
    "    train_signers = shuffled_signers[:n_train]\n",
    "    val_signers = shuffled_signers[n_train:n_train + n_val]\n",
    "    test_signers = shuffled_signers[n_train + n_val:]\n",
    "    \n",
    "    print(f\"\\nüìä Split by signer:\")\n",
    "    print(f\"   Train signers ({len(train_signers)}): {train_signers}\")\n",
    "    print(f\"   Val signers ({len(val_signers)}): {val_signers}\")\n",
    "    print(f\"   Test signers ({len(test_signers)}): {test_signers}\")\n",
    "    \n",
    "    # Create masks\n",
    "    train_mask = np.isin(signer_ids, train_signers)\n",
    "    val_mask = np.isin(signer_ids, val_signers)\n",
    "    test_mask = np.isin(signer_ids, test_signers)\n",
    "    \n",
    "    # Split data\n",
    "    X_train = X[train_mask]\n",
    "    y_train = y[train_mask]\n",
    "    X_val = X[val_mask]\n",
    "    y_val = y[val_mask]\n",
    "    X_test = X[test_mask]\n",
    "    y_test = y[test_mask]\n",
    "    \n",
    "    print(f\"\\nüìä Dataset sizes:\")\n",
    "    print(f\"   Train: {len(X_train)} videos\")\n",
    "    print(f\"   Val:   {len(X_val)} videos\")\n",
    "    print(f\"   Test:  {len(X_test)} videos\")\n",
    "    \n",
    "    # Check class distribution in splits\n",
    "    print(f\"\\nüéØ Classes in each split:\")\n",
    "    print(f\"   Train: {len(np.unique(y_train))} classes\")\n",
    "    print(f\"   Val:   {len(np.unique(y_val))} classes\")\n",
    "    print(f\"   Test:  {len(np.unique(y_test))} classes\")\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), {\n",
    "        'train_signers': train_signers.tolist(),\n",
    "        'val_signers': val_signers.tolist(),\n",
    "        'test_signers': test_signers.tolist()\n",
    "    }\n",
    "\n",
    "# Create splits\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test), signer_splits = create_signer_independent_splits(\n",
    "    X, y_encoded, signer_ids\n",
    ")\n",
    "\n",
    "# Save splits info\n",
    "with open('/content/signer_splits.json', 'w') as f:\n",
    "    json.dump(signer_splits, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de30a0a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 7: Normalize Features ---\n",
    "\n",
    "def normalize_features(X_train, X_val, X_test):\n",
    "    \"\"\"Z-score normalization per feature\"\"\"\n",
    "    \n",
    "    # Calculate mean and std from training data only\n",
    "    mean = np.mean(X_train, axis=(0, 1), keepdims=True)\n",
    "    std = np.std(X_train, axis=(0, 1), keepdims=True)\n",
    "    std[std == 0] = 1  # Avoid division by zero\n",
    "    \n",
    "    # Normalize\n",
    "    X_train_norm = (X_train - mean) / std\n",
    "    X_val_norm = (X_val - mean) / std\n",
    "    X_test_norm = (X_test - mean) / std\n",
    "    \n",
    "    print(f\"\\nüìä Normalization stats:\")\n",
    "    print(f\"   Mean shape: {mean.shape}\")\n",
    "    print(f\"   Std shape: {std.shape}\")\n",
    "    print(f\"   X_train range: [{X_train_norm.min():.2f}, {X_train_norm.max():.2f}]\")\n",
    "    \n",
    "    # Save normalization params\n",
    "    np.save('/content/normalization_mean.npy', mean)\n",
    "    np.save('/content/normalization_std.npy', std)\n",
    "    \n",
    "    return X_train_norm, X_val_norm, X_test_norm\n",
    "\n",
    "X_train_norm, X_val_norm, X_test_norm = normalize_features(X_train, X_val, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8fc8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 8: Save Prepared Data ---\n",
    "\n",
    "def save_prepared_data(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \"\"\"Save all prepared data for training\"\"\"\n",
    "    \n",
    "    # Create output folder\n",
    "    os.makedirs('/content/prepared_data', exist_ok=True)\n",
    "    \n",
    "    # Save features\n",
    "    np.save('/content/prepared_data/X_train.npy', X_train)\n",
    "    np.save('/content/prepared_data/X_val.npy', X_val)\n",
    "    np.save('/content/prepared_data/X_test.npy', X_test)\n",
    "    \n",
    "    # Save labels\n",
    "    np.save('/content/prepared_data/y_train.npy', y_train)\n",
    "    np.save('/content/prepared_data/y_val.npy', y_val)\n",
    "    np.save('/content/prepared_data/y_test.npy', y_test)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "        'sequence_length': X_train.shape[1],\n",
    "        'features_per_frame': X_train.shape[2],\n",
    "        'num_classes': len(np.unique(y_train)),\n",
    "        'normalized': True\n",
    "    }\n",
    "    \n",
    "    with open('/content/prepared_data/dataset_info.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Data saved to /content/prepared_data/\")\n",
    "    print(f\"   Train: {X_train.shape}\")\n",
    "    print(f\"   Val:   {X_val.shape}\")\n",
    "    print(f\"   Test:  {X_test.shape}\")\n",
    "\n",
    "save_prepared_data(X_train_norm, X_val_norm, X_test_norm, \n",
    "                  y_train, y_val, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e06828",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 9: Quick Data Quality Check ---\n",
    "\n",
    "def data_quality_check(X_train, y_train):\n",
    "    \"\"\"Check data quality\"\"\"\n",
    "    \n",
    "    print(\"\\nüîç DATA QUALITY CHECK\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check for NaN\n",
    "    has_nan = np.isnan(X_train).any()\n",
    "    print(f\"Contains NaN values: {'‚ùå YES' if has_nan else '‚úÖ NO'}\")\n",
    "    \n",
    "    # Check for Inf\n",
    "    has_inf = np.isinf(X_train).any()\n",
    "    print(f\"Contains Inf values: {'‚ùå YES' if has_inf else '‚úÖ NO'}\")\n",
    "    \n",
    "    # Check variance\n",
    "    variance = np.var(X_train, axis=(0, 1))\n",
    "    zero_var = np.sum(variance < 1e-6)\n",
    "    print(f\"Zero-variance features: {zero_var}/{X_train.shape[2]}\")\n",
    "    \n",
    "    # Class balance\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    min_class = counts.min()\n",
    "    max_class = counts.max()\n",
    "    print(f\"Class balance: min={min_class}, max={max_class}, ratio={max_class/min_class:.1f}\")\n",
    "    \n",
    "    # Sample sequence\n",
    "    sample_idx = np.random.randint(0, len(X_train))\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.plot(X_train[sample_idx, :, 0])  # Plot first feature over time\n",
    "    plt.title(f'Sample Sequence - Class {y_train[sample_idx]}')\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Feature Value')\n",
    "    plt.savefig('/content/sample_sequence.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "data_quality_check(X_train_norm, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a8c7e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 10: Create TensorFlow Dataset ---\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_tf_datasets(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=32):\n",
    "    \"\"\"Create TensorFlow datasets for training\"\"\"\n",
    "    \n",
    "    # Convert to one-hot\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "    y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train_cat))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val_cat))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test_cat))\n",
    "    \n",
    "    # Batch and prefetch\n",
    "    train_dataset = train_dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    print(f\"\\nüì¶ TensorFlow datasets created:\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Train batches: {len(list(train_dataset))}\")\n",
    "    print(f\"   Val batches: {len(list(val_dataset))}\")\n",
    "    print(f\"   Test batches: {len(list(test_dataset))}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset, num_classes\n",
    "\n",
    "train_ds, val_ds, test_ds, num_classes = create_tf_datasets(\n",
    "    X_train_norm, y_train, X_val_norm, y_val, X_test_norm, y_test\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203b930",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 11: Download Prepared Data ---\n",
    "\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Zip prepared data\n",
    "shutil.make_archive('/content/prepared_data', 'zip', '/content/prepared_data')\n",
    "files.download('/content/prepared_data.zip')\n",
    "\n",
    "print(\"‚úÖ Notebook 2 complete! Ready for model training.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
