{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba67ee08",
   "metadata": {},
   "source": [
    "# ===============================================================\n",
    "# ðŸ“’ NOTEBOOK 4: Model Optimization & Testing\n",
    "# Hyperparameter tuning, advanced optimization, final testing\n",
    "# ===============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a33f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1: Setup ---\n",
    "\n",
    "!pip install tensorflow pandas numpy matplotlib scikit-learn kerastuner\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"âœ… TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e3d27",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 2: Load Best Model ---\n",
    "\n",
    "def load_best_model():\n",
    "    \"\"\"Load the best model from Notebook 3\"\"\"\n",
    "    \n",
    "    model = keras.models.load_model('/content/models/best_model.h5')\n",
    "    \n",
    "    # Load data\n",
    "    X_test = np.load('/content/prepared_data/X_test.npy')\n",
    "    y_test = np.load('/content/prepared_data/y_test.npy')\n",
    "    \n",
    "    with open('/content/label_encoder.pkl', 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    \n",
    "    print(f\"âœ… Model loaded\")\n",
    "    print(f\"   Test data: {X_test.shape}\")\n",
    "    \n",
    "    return model, X_test, y_test, label_encoder\n",
    "\n",
    "model, X_test, y_test, label_encoder = load_best_model()\n",
    "\n",
    "# One-hot encode\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, len(label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affdb8d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 3: Hyperparameter Tuning (Optional) ---\n",
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "def build_tuning_model(hp):\n",
    "    \"\"\"Build model for hyperparameter tuning\"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=X_test.shape[1:]),\n",
    "        \n",
    "        # Conv1D layers\n",
    "        keras.layers.Conv1D(\n",
    "            filters=hp.Int('conv1_filters', 32, 128, step=32),\n",
    "            kernel_size=hp.Choice('conv1_kernel', [3, 5]),\n",
    "            padding='same',\n",
    "            activation='relu'\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        keras.layers.Conv1D(\n",
    "            filters=hp.Int('conv2_filters', 64, 256, step=64),\n",
    "            kernel_size=hp.Choice('conv2_kernel', [3, 5]),\n",
    "            padding='same',\n",
    "            activation='relu'\n",
    "        ),\n",
    "        keras.layers.MaxPooling1D(2),\n",
    "        \n",
    "        # LSTM or BiLSTM\n",
    "        if hp.Boolean('use_bilstm'):\n",
    "            keras.layers.Bidirectional(\n",
    "                keras.layers.LSTM(\n",
    "                    units=hp.Int('lstm_units', 64, 256, step=64),\n",
    "                    return_sequences=False\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            keras.layers.LSTM(\n",
    "                units=hp.Int('lstm_units', 64, 256, step=64),\n",
    "                return_sequences=False\n",
    "            ),\n",
    "        \n",
    "        # Dense layers\n",
    "        keras.layers.Dense(\n",
    "            units=hp.Int('dense_units', 128, 512, step=128),\n",
    "            activation='relu'\n",
    "        ),\n",
    "        keras.layers.Dropout(\n",
    "            rate=hp.Float('dropout', 0.2, 0.5, step=0.1)\n",
    "        ),\n",
    "        \n",
    "        keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "        ),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Uncomment to run tuning (takes hours)\n",
    "\"\"\"\n",
    "tuner = kt.RandomSearch(\n",
    "    build_tuning_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='/content/tuning',\n",
    "    project_name='slsl_tuning'\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train, y_train_cat,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(5)\n",
    "print(best_hps[0].values)\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… Hyperparameter tuning ready (uncomment to run)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d4a3fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 4: Ensemble Prediction ---\n",
    "\n",
    "def ensemble_predict(models, X):\n",
    "    \"\"\"Predict using ensemble of models\"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        pred = model.predict(X, verbose=0)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Average predictions\n",
    "    ensemble_pred = np.mean(predictions, axis=0)\n",
    "    return ensemble_pred\n",
    "\n",
    "# Load multiple models (if available)\n",
    "models = [model]  # Add more models if you have them\n",
    "\n",
    "if len(models) > 1:\n",
    "    y_pred_ensemble = np.argmax(ensemble_predict(models, X_test), axis=1)\n",
    "    y_true = np.argmax(y_test_cat, axis=1)\n",
    "    \n",
    "    ensemble_acc = np.mean(y_pred_ensemble == y_true)\n",
    "    print(f\"\\nðŸŽ¯ Ensemble Accuracy: {ensemble_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73144845",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 5: Confidence Calibration ---\n",
    "\n",
    "def calibrate_confidence(y_pred_probs, y_true):\n",
    "    \"\"\"Analyze model confidence\"\"\"\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    confidences = np.max(y_pred_probs, axis=1)\n",
    "    \n",
    "    # Correct predictions\n",
    "    correct_mask = (y_pred == y_true)\n",
    "    correct_conf = confidences[correct_mask]\n",
    "    wrong_conf = confidences[~correct_mask]\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Histogram of confidences\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist([correct_conf, wrong_conf], bins=20, \n",
    "             label=['Correct', 'Wrong'], alpha=0.7)\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Confidence Distribution')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Reliability diagram\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bins = np.linspace(0, 1, 11)\n",
    "    correct_rates = []\n",
    "    for i in range(len(bins)-1):\n",
    "        mask = (confidences >= bins[i]) & (confidences < bins[i+1])\n",
    "        if np.sum(mask) > 0:\n",
    "            acc = np.mean(y_pred[mask] == y_true[mask])\n",
    "            correct_rates.append(acc)\n",
    "        else:\n",
    "            correct_rates.append(0)\n",
    "    \n",
    "    plt.plot(bins[:-1] + 0.05, correct_rates, 'o-', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], '--', label='Perfect')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Reliability Diagram')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/calibration.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "calibrate_confidence(y_pred_probs, np.argmax(y_test_cat, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf36e52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 6: Test on New Videos ---\n",
    "\n",
    "def test_on_new_video(video_path, model, label_encoder):\n",
    "    \"\"\"Test model on a completely new video\"\"\"\n",
    "    \n",
    "    # This would require the landmark extraction pipeline\n",
    "    # You can implement this if you have new test videos\n",
    "    \n",
    "    print(\"ðŸ”„ To test new videos, first run them through Notebook 1\")\n",
    "    print(\"   Then load the landmarks and run inference\")\n",
    "    \n",
    "    # Placeholder for implementation\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f073b82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 7: Final TFLite Optimization ---\n",
    "\n",
    "def optimize_tflite_final(model):\n",
    "    \"\"\"Apply advanced TFLite optimizations\"\"\"\n",
    "    \n",
    "    # Try different quantization methods\n",
    "    converters = {\n",
    "        'float32': tf.lite.TFLiteConverter.from_keras_model(model),\n",
    "        'float16': tf.lite.TFLiteConverter.from_keras_model(model),\n",
    "        'int8': tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    }\n",
    "    \n",
    "    # Float32 (baseline)\n",
    "    converters['float32'].optimizations = []\n",
    "    \n",
    "    # Float16 quantization\n",
    "    converters['float16'].optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converters['float16'].target_spec.supported_types = [tf.float16]\n",
    "    \n",
    "    # Int8 quantization (requires representative dataset)\n",
    "    def representative_dataset():\n",
    "        for i in range(100):\n",
    "            yield [X_test[i:i+1].astype(np.float32)]\n",
    "    \n",
    "    converters['int8'].optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converters['int8'].representative_dataset = representative_dataset\n",
    "    converters['int8'].target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converters['int8'].inference_input_type = tf.int8\n",
    "    converters['int8'].inference_output_type = tf.int8\n",
    "    \n",
    "    results = {}\n",
    "    for name, converter in converters.items():\n",
    "        try:\n",
    "            tflite_model = converter.convert()\n",
    "            size = len(tflite_model) / 1024\n",
    "            results[name] = {\n",
    "                'model': tflite_model,\n",
    "                'size_kb': size,\n",
    "                'path': f'/content/models/model_{name}.tflite'\n",
    "            }\n",
    "            \n",
    "            # Save\n",
    "            with open(results[name]['path'], 'wb') as f:\n",
    "                f.write(tflite_model)\n",
    "            \n",
    "            print(f\"{name}: {size:.2f} KB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name}: Failed - {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "tflite_results = optimize_tflite_final(model)\n",
    "\n",
    "# Compare sizes\n",
    "print(\"\\nðŸ“Š Model Size Comparison:\")\n",
    "for name, result in tflite_results.items():\n",
    "    print(f\"   {name}: {result['size_kb']:.2f} KB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b1303",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 8: Speed Benchmark ---\n",
    "\n",
    "def benchmark_tflite(tflite_path, X_test, num_runs=100):\n",
    "    \"\"\"Benchmark TFLite inference speed\"\"\"\n",
    "    \n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Warmup\n",
    "    for i in range(10):\n",
    "        interpreter.set_tensor(input_details[0]['index'], X_test[i:i+1].astype(np.float32))\n",
    "        interpreter.invoke()\n",
    "    \n",
    "    # Benchmark\n",
    "    import time\n",
    "    times = []\n",
    "    \n",
    "    for i in range(min(num_runs, len(X_test))):\n",
    "        interpreter.set_tensor(input_details[0]['index'], X_test[i:i+1].astype(np.float32))\n",
    "        \n",
    "        start = time.time()\n",
    "        interpreter.invoke()\n",
    "        end = time.time()\n",
    "        \n",
    "        times.append((end - start) * 1000)  # ms\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    print(f\"\\nâš¡ Speed Benchmark ({num_runs} runs):\")\n",
    "    print(f\"   Average: {avg_time:.2f} ms\")\n",
    "    print(f\"   Std Dev: {std_time:.2f} ms\")\n",
    "    print(f\"   Min: {np.min(times):.2f} ms\")\n",
    "    print(f\"   Max: {np.max(times):.2f} ms\")\n",
    "    print(f\"   FPS: {1000/avg_time:.1f} fps\")\n",
    "    \n",
    "    return times\n",
    "\n",
    "# Benchmark best model\n",
    "best_model_path = '/content/models/sentence_model.tflite'\n",
    "times = benchmark_tflite(best_model_path, X_test)\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(times, bins=20, alpha=0.7)\n",
    "plt.axvline(np.mean(times), color='red', linestyle='--', label=f'Mean: {np.mean(times):.2f}ms')\n",
    "plt.xlabel('Inference Time (ms)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Inference Time Distribution')\n",
    "plt.legend()\n",
    "plt.savefig('/content/benchmark.png', dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32edc0fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 9: Final Model Report ---\n",
    "\n",
    "def generate_final_report(model, tflite_results, test_accuracy, times):\n",
    "    \"\"\"Generate comprehensive model report\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'model_info': {\n",
    "            'input_shape': str(model.input_shape),\n",
    "            'output_shape': str(model.output_shape),\n",
    "            'total_params': model.count_params(),\n",
    "            'trainable_params': sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "        },\n",
    "        'performance': {\n",
    "            'test_accuracy': float(test_accuracy),\n",
    "            'inference_time_ms': float(np.mean(times)),\n",
    "            'inference_std_ms': float(np.std(times)),\n",
    "            'fps': float(1000/np.mean(times))\n",
    "        },\n",
    "        'tflite_models': {\n",
    "            name: {\n",
    "                'size_kb': float(result['size_kb']),\n",
    "                'path': result['path']\n",
    "            } for name, result in tflite_results.items()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    with open('/content/final_model_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    # Create text report\n",
    "    with open('/content/final_model_report.txt', 'w') as f:\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"SLSL TRANSLATION MODEL - FINAL REPORT\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"MODEL ARCHITECTURE:\\n\")\n",
    "        f.write(f\"  Input Shape: {report['model_info']['input_shape']}\\n\")\n",
    "        f.write(f\"  Total Parameters: {report['model_info']['total_params']:,}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PERFORMANCE:\\n\")\n",
    "        f.write(f\"  Test Accuracy: {report['performance']['test_accuracy']*100:.2f}%\\n\")\n",
    "        f.write(f\"  Inference Time: {report['performance']['inference_time_ms']:.2f} ms\\n\")\n",
    "        f.write(f\"  FPS: {report['performance']['fps']:.1f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"TFLITE MODELS:\\n\")\n",
    "        for name, res in report['tflite_models'].items():\n",
    "            f.write(f\"  {name}: {res['size_kb']:.2f} KB\\n\")\n",
    "    \n",
    "    print(\"\\nðŸ“„ Final report saved to /content/final_model_report.txt\")\n",
    "    return report\n",
    "\n",
    "# Get test accuracy\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "report = generate_final_report(model, tflite_results, test_acc, times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b6cc0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 10: Prepare for Publication ---\n",
    "\n",
    "def prepare_publication_package():\n",
    "    \"\"\"Prepare all files for publication/submission\"\"\"\n",
    "    \n",
    "    os.makedirs('/content/publication_package', exist_ok=True)\n",
    "    \n",
    "    # Copy all important files\n",
    "    !cp /content/models/sentence_model.tflite /content/publication_package/\n",
    "    !cp /content/final_model_report.txt /content/publication_package/\n",
    "    !cp /content/training_history.png /content/publication_package/\n",
    "    !cp /content/confusion_matrix.png /content/publication_package/\n",
    "    !cp /content/per_class_accuracy.png /content/publication_package/\n",
    "    !cp /content/calibration.png /content/publication_package/\n",
    "    !cp /content/benchmark.png /content/publication_package/\n",
    "    !cp /content/label_encoder.pkl /content/publication_package/\n",
    "    \n",
    "    # Create README\n",
    "    with open('/content/publication_package/README.txt', 'w') as f:\n",
    "        f.write(\"SLSL MEDICAL TRANSLATION MODEL\\n\")\n",
    "        f.write(\"=\"*40 + \"\\n\\n\")\n",
    "        f.write(\"Files included:\\n\")\n",
    "        f.write(\"- sentence_model.tflite: Quantized model for mobile\\n\")\n",
    "        f.write(\"- label_encoder.pkl: Label mapping\\n\")\n",
    "        f.write(\"- final_model_report.txt: Detailed performance report\\n\")\n",
    "        f.write(\"- *.png: Performance visualizations\\n\\n\")\n",
    "        f.write(\"To use in Flutter:\\n\")\n",
    "        f.write(\"1. Copy sentence_model.tflite to assets/models/\\n\")\n",
    "        f.write(\"2. Use label_encoder.pkl to map predictions to Sinhala text\\n\")\n",
    "    \n",
    "    # Zip everything\n",
    "    !zip -r /content/slsl_publication_package.zip /content/publication_package/\n",
    "    \n",
    "    print(\"\\nðŸ“¦ Publication package ready: /content/slsl_publication_package.zip\")\n",
    "\n",
    "prepare_publication_package()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730306f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 11: Download Final Package ---\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files.download('/content/slsl_publication_package.zip')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ ALL NOTEBOOKS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâœ… Model ready for Flutter integration\")\n",
    "print(\"âœ… Performance reports generated\")\n",
    "print(\"âœ… TFLite optimized for mobile\")\n",
    "print(\"\\nðŸ“± Next: Use in Flutter app with tflite_flutter\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
